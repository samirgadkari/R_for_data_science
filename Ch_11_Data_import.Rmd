---
title: "Ch_11_Data_import"
author: "Samir Gadkari"
date: "1/18/2021"
output: html_document
---

```{r}
library(tidyverse)
library(hms) # for representing time in hh:mm:ss am/pm
library(feather) # file format common across programming
                 # languages. Save tibbles in this format to
                 # give to other users.
read_path = Sys.getenv('DATASETS')
write_path = paste(read_path, 'output', sep = '/')
working_dir = Sys.getenv('R_WORKING_DIR')

full_read_path <- function(filename) {
  paste(read_path, filename, sep = '/')
}

full_write_path <- function(filename) {
  paste(write_path, filename, sep = '/')
}

full_wd_path <- function(filename) {
  paste(working_dir, filename, sep = '/')
}
```

## 11.2 Getting started

  * read_csv reads comma separated files
  * read_csv2 reads semicolon separated files
  * read_tsv reads tab separated files
  * read_delim reads files with any delimeter
  * read_fwf reads fixed-width files
    - fwf_widths or fwf_positions give either the widths or
      positions to read_fwf as the second argument after the
      file handle
  * read_log reads Apache-style log files.
    Also check out webreadr which is built on top of read_log
    and provides many more helpful tools

```{r}
mens_shoe_prices <-
  read_csv(full_read_path('mens_shoe_prices.csv'))
```
You can see how read_csv shows a column specification for the
data it just read.

If there are any problems, you can look at them using the
problems function on the tibble.

To create an example tibble:
```{r}
# First line of data are the column names
read_csv('a, b, c
1, 2, 3
4, 5, 6')
```

There are two cases when reading from a delimited file:
1. A few lines of metadata at the top of the file should be
removed.

You can use:
  * skip = n to skip n lines at the top of the file
  * comment = '#' to drop lines starting with #
```{r}
read_csv('This is a comment
and so is this
x, y, z
1, 2, 3', skip = 2)

read_csv('# this is a comment
# which should be skipped
a, b, c
1, 2, 3', comment = '#')
```

2. The data might not have column names. You have to:
  * Use the read_csv function with col_names = FALSE to
    start reading data from the first line in the file.
    This also provides temporary names for each column (
    X1, X2, ... Xn)
```{r}
read_csv('1, 2, 3\n4, 5, 6', col_names = FALSE)
```
    
  * Specify column names using col_names = c('A', 'B', 'C')
```{r}
read_csv('1, 2, 3\n4, 5, 6', col_names = c('A', 'B', 'C'))
```

To handle NA values, you can specify na = with value or values
that will be replaced with NA:
```{r}
read_csv('1, 2, na\n4, ., 6', col_names = FALSE, 
         na = '.')
read_csv('1, 2, na\n4, ., 6', col_names = FALSE, 
         na = c('na', '.'))
```
To read in more challenging files, look at readr documentation.

## 11.2.1 Compared to base R
Why do we use readr package's read_csv 
instead of base R's read.csv?

  * read_csv is approximately 10x faster than base R's read.csv.
  
    data.table::fread() is fastest, but does not fit well
    with tidyverse
  * readr functions
    + produce tibbles
    + don't convert character vectors to factors
    + don't use row names (like index in pandas).
    
      When you convert a data.frame to a tibble, it will
      have row names. These functions help you process the
      row names of a data.frame
      - has_rownames(df)
      - remove_rownames(df)
      - rownames_to_column(df, var = 'rowname')
      - rowid_to_column(df, var = 'rowid')
      
        adds a column at the start of a dataframe with
        ascending sequential numbers in it
      - column_to_rownames(df, var = 'rowname')
    + don't munge column names
  * they're more reproducible. 
    
    Base R uses some environment variables and behavior
    from your operating system. This means code that works 
    on your computer may not work on someone else's computer.

## 11.2.2 Exercises

1. What function would you use to read a file where fields were separated with “|”?

read_delim

2. Apart from file, skip, and comment, what other arguments do read_csv() and read_tsv() have in common?

All arguments are the same between the two. These are the
other arguments:
col_names, col_types, locale, na, progress, skip_empty_rows
Other arguments with explanations:
guess_max: max number of records used to guess column types
n_max:     max number of records to read
quote:     single character used to quote strings
quoted_na: should missing values inside quotes be treated
           as missing values (default) or strings
trim_ws:   should leading/trailing whitespace be trimmed?

3. What are the most important arguments to read_fwf()?

file, col_types, na, trim_ws, skip, skip_empty_rows
Other arguments with explanations:
col_positions: column positions created by fwf_widths, 
               fwf_positions, or fwf_empty
               
4. Sometimes strings in a CSV file contain commas. To prevent them from causing problems they need to be surrounded by a quoting character, like " or '. By default, read_csv() assumes that the quoting character will be ". What argument to read_csv() do you need to specify to read the following text into a data frame?

"x,y\n1,'a,b'"
```{r}
read_csv("x,y\n1,'a,b'", quote = "\'")
```

5. Identify what is wrong with each of the following inline CSV files. What happens when you run the code?
```{r}
# creates a 2 x 2 table - does not include 3, 6 since
# there are not enough column names in the top row
read_csv("a,b\n1,2,3\n4,5,6")

# 2 columns specified in the top row, but
# 2 values in first data row, so last column value is NA, and
# 4 values in second data row, so last value ignored.
read_csv("a,b,c\n1,2\n1,2,3,4")

# 2 columns specified in top row, 
# 1 value in second row, so second column has value NA.
# Also since quote is not specified, quotes are dropped,
# and value of 1 <dbl> is used
read_csv("a,b\n\"1")

# 2 columns specified in top row,
# second row could be <dbl>, but
# third row are all characters.
# Since columns need to have the same type,
# all columns are <chr>
read_csv("a,b\n1,2\na,b")

# Should have used read_delim with delim = ';'.
# Top row is combined into one name 'a;b', and
# first data row is combined into one '1;3', so
# the type of the column is character.
read_csv("a;b\n1;3")
```

## 11.3 Parsing a vector

parse_* functions take a character vector and return a more
specialised vector like a logical, integer, or date.
```{r}
str(parse_logical(c('TRUE', 'FALSE', 'TRUE')))
str(parse_integer(c('1', '2', '3')))
str(parse_date(c('2010-01-01', '2020-01-01')))
```
parse_* functions have na argument which converts those strings
into NA values:
```{r}
parse_integer(c('1', '123', '.', '256'), na = '.')
```

If parsing fails, you get a warning, and the failures will be
missing from the output:
```{r}
x <- parse_integer(c('123', '456', 'abc', '45.26'))
```

If there are many failures, you will need to use problems(df)
to get the complete set:
```{r}
problems(x) # returns a tibble
```

Eight important parsers:

  * parse_logical, parse_integer
  * Numeric parsers
  
    parse_double:  strict numeric parser
    
    parse_number:  flexible numeric parser
    
    These are more complicated because numbers are written
    differently in different countries.
    
  * Character parsers
  
    parse_character: encodings make this complicated
    
  * Factor parsers
  
    parse_factor: creates factors to represent categories
    
  * Date and time parsers
  
    These allow you to parse various types of dates and times
    
    parse_datetime 
    
    parse_date
    
    parse_time

The following sections describe these parsers in detail.

## 11.3.1 Numbers

This is tricky because:

  * In some countries the integer and fractional parts are
    separated by , instead of by . Use locale to fix this:
    
```{r}
parse_number('1.23')
parse_number('1,23', locale = locale(decimal_mark = ','))
```
readr's locale defaults to US, since R is US-centric.  
  
  * Numbers are often surrounded by other characters like
    $ or %. parse_number ignores extra characters before and
    after the numeric characters:
    
```{r}
parse_number('$100')
parse_number('87%')
parse_number('It cost $123.48')
```

    
  * Numbers often contain grouping characters like , 
    ex: 1,000,000. parse_number ignores grouping characters.
    You can use locale if the grouping character is different
    from the default ,:
```{r}
parse_number('$1,234,567')

# In Europe, grouping mark of , and decimal mark of .
# are used. Here the output has this number 
# rounded up
parse_number('123.456.789,89', 
             locale = locale(grouping_mark = '.',
                             decimal_mark = ','))

# The swiss use ' as a grouping mark
parse_number("123'456'789",
             locale = locale(grouping_mark = "'"))
```
    
## 11.3.2 Strings
```{r}
charToRaw('Hadley') # shows hex byte values of string
```
By default, strings are encoded in ASCII in R.
The readr library uses UTF-8 everywhere. This is a good default,
but will fail for data used by older systems.
```{r}
(x1 <- "El Ni\xf1o was particularly bad this year")
(x2 <- "\x82\xb1\x82\xf1\x82\xc9\x82\xbf\x82\xcd")
parse_character(x1, locale = locale(encoding = 'Latin1'))
parse_character(x2, locale = locale(encoding = 'Shift-JIS'))
```
readr provides guess_encoding function which guesses the
encoding by reading the bytes in the file. It's not foolproof,
but it works better if there are more characters for it to read
in the file.
```{r}
guess_encoding(charToRaw(x1))
guess_encoding(charToRaw(x2))
```
More encoding information at:
[]('http://kunststube.net/encoding/')

## 11.3.3 Factors

Give the parse_factor function a vector of known levels to
produce an error when an unknown factor is present
```{r}
fruit <- c('apple', 'banana')
parse_factor(c('apple', 'banana', 'bananana'),
             levels = fruit)
```
If there are many errors, it is easier to keep the column
as a string, and use the tools in Ch 14 (Strings) and
Ch 15 (Factors) to clean them up.

## 11.3.4 Dates, date-times, and times

* parse_datetime

  + returns number of days since 1970-01-01
  
  + expects an ISO8601 date-time
  
    yyyy-mm-ddThhmmss (the T should appear as-is in the string).
    If time is omitted, it will be midnight the same day.

```{r}
parse_datetime('2010-10-01T2010')
```

* parse_date

  + expects a four digit year, a - or /, the month, 
    a - or /, then the day

  + returns number of seconds since 1970-01-01
  
```{r}
parse_date('2010-10-01')
```

* parse_time

  + expects the hour, :, minutes, optionally : and seconds, 
    and an optional am/pm specifier
    
  + returns number of seconds since midnight

```{r}
parse_time('01:10 am')
parse_time('20:10:01')
```

If these are not enough, you can pass a format to
parse_date and parse_datetime:

  * Year: %Y (4 digits), %y (2 digits)
  
  * Month: %m (2 digits), %b (ex. 'Jan'), %B (ex. 'January')
  
  * Day: %d (2 digits), %e (optional leading space)
  
  * Time: 
  
    - %H (0-23 hour), 
          
    - %I (0-12 hour, must use %p),
    - %p (AM/PM indicator),
    - %M (minutes),
    - %S (integer seconds),
    - %OS (real seconds),
    - %Z (time zone, ex. America/Chicago),
    - %z (offset from UTC, ex. +0800),
    - %. (skip one non-digit character when parsing),
    - %* (skips any number of non-digits when parsing)
```{r}
parse_date('01/02/15', '%m/%d/%y')
parse_date('01/02/15', '%d/%m/%y')
parse_date('01/02/15', '%y/%m/%d')
parse_date("1 janvier 2015", "%d %B %Y", 
           locale = locale("fr"))
```

## 11.3.5 Exercises

1. What are the most important arguments to locale()?

decimal_mark, grouping_mark, encoding

2. What happens if you try and set decimal_mark and grouping_mark to the same character? What happens to the default value of grouping_mark when you set decimal_mark to “,”? What happens to the default value of decimal_mark when you set the grouping_mark to “.”?

3. I didn’t discuss the date_format and time_format options to locale(). What do they do? Construct an example that shows when they might be useful.

date_format and time_format parameters to locale might be
useful when the default date/time formats do not match
the one in the data.

7. Generate the correct format string to parse each of the following dates and times:
```{r}
d1 <- "January 1, 2010"
parse_date(d1, '%B%.%d%*%Y')

d2 <- "2015-Mar-07"
parse_date(d2, '%Y%.%b%.%d')

d3 <- "06-Jun-2017"
parse_date(d3, '%d%.%b%.%Y')

d4 <- c("August 19 (2015)", "July 1 (2015)")
parse_date(d4, '%B%.%d%*%Y%.')

d5 <- "12/30/14" # Dec 30, 2014
parse_date(d5, '%m%.%d%.%y')

t1 <- "1705"
parse_date(t1, '%Y')

t2 <- "11:15:10.12 PM"
parse_time(t2, '%I%.%M%.%OS%.%p')
```
## 11.4 Parsing a file

In this section, you will learn:

  * How does readr automatically guess the type of each column.

  * How to override the default specification.

### 11.4.1 Strategy

readr uses heuristics on the first 1000 rows to guess 
column types. The guess_parser function returns readr's
best guess, the parse_guess function uses that guess to
parse the column.
```{r}
guess_parser('2010-10-01')
guess_parser(c('TRUE', 'FALSE'))
guess_parser('15:01')
guess_parser(c('1', '5', '9'))
guess_parser('109,258,350')

str(parse_guess('2010-10-01'))
```

The heuristic tries each of the following types, stopping when it finds a match:

    * logical: contains only “F”, “T”, “FALSE”, or “TRUE”.
    
    * integer: contains only numeric characters (and -).
    
    * double: contains only valid doubles (including numbers
              like 4.5e-5).
    
    * number: contains valid doubles with the
              grouping mark inside.
    
    * time: matches the default time_format.
    
    * date: matches the default date_format.
    
    * date-time: any ISO8601 date.

If none of these rules apply, then the column will stay as a vector of strings.

### 11.4.2 Problems

For larger files, there are two main problems:

  * The first 1000 rows may be a special case. ex:
    You have a column of doubles, but the first 1000 are
    integers
  
  * If the first 1000 rows contain only NAs, readr will guess
    that it's a logical vector
    
readr contains a challenging csv which will show you both
these problems.
```{r}
challenge <- read_csv(readr_example('challenge.csv'))
```
```{r}
problems(challenge)
```
A good way is to fix each issue a column at a time.
Let's fix it for column y:
```{r}
tail(challenge)
```
We can see that the x column are made of doubles, and that
many of y's values are NA, while the actual
values shown for rows from 1001 are dates. Let's use the
col_date specification for the y column:
```{r}
challenge <- read_csv(
  readr_example('challenge.csv'),
  col_types = cols(
    x = col_double(),
    y = col_date()
  )
)

tail(challenge)
```
Each parse_* function has a corresponding col_* function.

  * parse_* functions are used to read from strings.

  * col_* functions are used by the file parser.
  
Always use col_* functions when reading files. This ensures
that the data is of the correct type when you have to use it
later on in the code. If the given file has the wrong type,
you will error out, instead of going through part of your code,
and not know why it does not work. Use stop_for_problems
function to guarantee readr will stop if it encounters a problem
reading a file.

### 11.4.3 Other strategies

The challenge.csv file is made to show readr problems.
Even with that file, if we have used just one more row
to parse, we would have the right data in a tibble:
```{r}
challenge2 <- read_csv(readr_example("challenge.csv"), 
                       guess_max = 1001)
```

You can also read in all columns as character vectors,
and then use the type_convert function to update the types:
```{r}
(df <- tribble(
  ~x, ~y,
  '1', '1.32',
  '2', '.2',
  '3', '3.33'
))

type_convert(df)
```

For very large files, set n_max to a smaller number (say
10,000 or 100,000) until you can read the file correctly.

For hard parsing problems, read each line in a file into
a character string using the read_lines function.
You can even use the read_file function to read the file
into a character vector of length 1.
```{r}
df <- read_file(readr_example('challenge.csv'))
View(df)
```

## Writing to a file

Use readr::write_csv and readr::write_tsv to write to disk.
Make sure to:

  * Always encode strings in UTF-8 before writing
  
  * Always encode dates/times in ISO8601 format
  
This way, the files can be easily read in later.

To export to Excel, use the write_excel_csv function.
This tells Excel that you're using UTF-8 encoding.

```{r}
write_csv(challenge, full_wd_path('challenge.csv'))
read_csv(full_wd_path('challenge.csv'))
```
Note that type information is lost when you save to a csv.
So csv's are not a good way to cache interim results, since
you have to parse them on each read. Instead:

  1. Use read_rds and write_rds functions to save in R's
     custom binary format. This way you don't lose
     column type information
```{r}
write_rds(challenge, full_wd_path('challenge.csv'))
read_rds(full_wd_path('challenge.csv'))
```
    2. Use feather package's write_feather, read_feather
       functions
       
       feather is faster than RDS, and is usable outside R.
       But RDS supports list columns, which feather does not.
```{r}
write_feather(challenge, full_wd_path('challenge.csv'))
read_feather(full_wd_path('challenge.csv'))
```

## 11.6 Other types of data

For other types of rectangular data:

    * haven reads SPSS, Stata, and SAS files.

    * readxl reads excel files (both .xls and .xlsx).

    * DBI, along with a database specific backend 
      (e.g. RMySQL, RSQLite, RPostgreSQL etc) 
      allows you to run SQL queries against a database 
      and return a data frame.

For hierarchical data: 

    * use jsonlite (by Jeroen Ooms) for json, and 
    
    * xml2 for XML. 
    
Jenny Bryan has some excellent worked examples at https://jennybc.github.io/purrr-tutorial/.

For other file types, try the [R data import/export manual]('https://cran.r-project.org/doc/manuals/r-release/R-data.html') and the rio package.
