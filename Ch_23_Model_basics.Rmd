---
title: "Ch_23_Model_basics"
author: "Samir Gadkari"
date: "2/10/2021"
output: html_document
---

```{css, echo = FALSE}
body {
  background-color: black;
  filter: invert(1);
}
table {
  empty-cells: hide;
}
```
```{r results='hide'}
library(tidyverse)
library(modelr)
library(ggplot2)

# We want to get a warning if there are any NA values when
# we run a function.
options(na.action = na.warn)
```

The goal of a model is to provide a simple low-dimensional summary 
of a dataset. In the context of this book we’re going to use models 
to partition data into patterns and residuals. Strong patterns will
hide subtler trends, so we’ll use models to help peel back layers 
of structure as we explore a dataset.

George Box said: "All models are wrong, but some are useful."
He mentions that a cunningly chosen frugal model often provides
useful approximations to the data.

## 23.2 A simple model

Let's look at the sim1 dataset from the modelr package:
```{r}
ggplot(sim1, aes(x, y)) +
  geom_point()
```

Let's create some models with random slope and intercept:
```{r}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) +
  geom_abline(aes(intercept = a1, slope = a2), 
              data = models, 
              alpha = 1 / 4) +
  geom_point()
```

Let's find the best lines using the distance from the line to each
point:
```{r}
model1 <- function(a, data) {
  a[1] + a[2] * data$x
}

measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff^2))
}

sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

models <- models %>%
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
models
```

```{r}
ggplot(sim1, aes(x, y)) +
  geom_point(size = 2, color = "grey30") +
  geom_abline(
    aes(intercept = a1, slope = a2, color = -dist),
    data = filter(models, rank(dist) <= 10)
  )
```

We can look at these models as points on a graph of a1 vs a2:
```{r}
ggplot(models, aes(a1, a2)) +
  geom_point(data = filter(models, rank(dist) <= 10),
             size = 4, color = 'red') +
  geom_point(aes(color = -dist))
```

Instead of slope and intercept coefficients created randomly,
let's create them on a grid:
```{r}
grid <- expand.grid(  # Creates a df from all combinations of
                      # parameters provided to it.
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)
  ) %>%
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>%
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10),
             size = 4, color = "red") +
  geom_point(aes(color = -dist))
```

Overlaying the best 10 models on the data:
```{r}
ggplot(sim1, aes(x, y)) +
  geom_point(size = 2, color = "grey20") +
  geom_abline(aes(intercept = a1, slope = a2, color = -dist),
              data = filter(grid, rank(dist) <= 10))
```

All of them look like a close match. You can make the grid 
finer and finer until you get an exact match. A better way is to
use the Newton-Raphson search. It goes down the slope of the
error surface until the change in error is low. We can use
optim function to do that:
```{r}
best <- optim(c(0, 0), # initial values for params to optimize over
              measure_distance, # function to calculate error
              data = sim1)
best$par
```
```{r}
ggplot(sim1, aes(x, y)) +
  geom_point(size = 2, color = "grey30") +
  geom_abline(aes(intercept = best$par[1], slope = best$par[2]))
```

There’s one more approach that we can use for this model, 
because it’s a special case of a broader family: linear models. 
A linear model has the general form 
y = a_1 + a_2 * x_1 + a_3 * x_2 + ... + a_n * x_(n - 1).

R has a tool specifically designed for fitting linear models 
called lm(). lm() has a special way to specify the model family:
formulas. Formulas look like y ~ x, which lm() will translate 
to a function like y = a_1 + a_2 * x.

```{r}
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
```

lm function actually finds the closest model in a single step.
It is faster than optim, and guarantees a global minimum.

### 23.2.1 Exercises

1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?
```{r}
x = rep(1:10, each = 3)
y = x * 1.5 + 6 + rt(length(x),  # rt generates t-style variates
                                 # with length given by param 1.
                     df = 2)     # Degrees of freedom.

# To simulate an unusual value, let's change the value of the end
# of the model to beyond 75th percentile + 3 * IQR. 
# Usually, 75th percentile + 1.5 * IQR is used to determine outliers.
y[[length(y)]] <- quantile(y)[[4]] + 15 * IQR(y)

sim1a <- tibble(
  x = x,
  y = y
)

sim1a_mod <- lm(y ~ x, data = sim1a)
ggplot(sim1a, aes(x, y)) +
  geom_point(color = "grey30") +
  geom_abline(aes(intercept = coef(sim1a_mod)[[1]],
                  slope = coef(sim1a_mod)[[2]]))
```

You can see that the model is pulling the line up at the right because
of the outlier. Let's see how much it is off by limiting the y range
of the plot:
```{r}
ggplot(sim1a, aes(x, y)) +
  geom_point(color = "grey30") +
  geom_abline(aes(intercept = coef(sim1a_mod)[[1]],
                  slope = coef(sim1a_mod)[[2]])) +
  coord_cartesian(ylim = c(-20, 40))
```
Now you can actually see how off the line is.

Note that we had to push the outlier a lot. Usually, the formula
for finding an outlier is:

  * anything higher than 75th percentile + 1.5 * IQR or
  * anything lower than 75th percentile - 1.5 * IQR

We used a large outlier because it is not visible to see an
outlier around 1.5 * IQR.

We had a single value that was high. Imagine if there were more
outliers, how the slope would change. Instead of the squared
error, when faced with outliers, you can choose to use 
absolute value of the error. This way the line is not pulled
so much.

2. One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance. Use optim() to fit this model to the simulated data above and compare it to the linear model.
```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  mean(abs(diff))
}

best <- optim(c(0, 0), measure_distance, data = sim1a)

coef(sim1a)
ggplot(sim1a, aes(x, y)) +
  geom_point(color = "grey30", size = 2) +
  geom_abline(aes(intercept = best$par[1], 
                  slope = best$par[2]),
              color = "red") +
  geom_abline(aes(intercept = coef(sim1a_mod)[[1]],
                  slope = coef(sim1a_mod)[[2]]),
              color = "green")
```
The red line (using the mean-absolute distance) is pulled less by the outlier. Thus it is a better fit to the other points.

3. One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optimum. What’s the problem with optimising a three parameter model like this?
```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}
```
In the above model, a[1] + a[3] is the intercept. How much of the intercept value is split between a[1] and a[3] cannot be determined.
If there was another variable multiplying a[3], like say x^2,
then we could find the individual values of the best a[1] and a[3].

## 23.3 Visualizing models

Once you have a model you can:

  * look at the model coefficients and the equation to understand
    the pattern the model captured
  * understand the model by looking at it's predictions.
    This is the pattern that the model has captured.
  * understand the model by looking at it's residuals.
    Residuals are the amounts of data the model has not captured.
    This allows us to remove the larger patterns in a dataset
    to get to the subtler variations.
    
### 23.3.1 Predictions

Let's generate an evenly-spaced grid of values that covers where
our data lies. We use modelr::data_grid for this:
```{r}
grid <- sim1 %>%
  data_grid(x)
grid
```

Then we add predictions to the model using modelr::add_predictions.
```{r}
grid <- grid %>%
  add_predictions(sim1_mod) # adds predictions to a dataset by
                            # calculating y values at given x values.
grid
```

Next we plot the predictions. The reason we do this instead of using
geom_abline is that we can use this for all models, not just
the linear models.

For more ideas on visualizing complex models, see:
[Visualizing complex models](http://vita.had.co.nz/papers/model-vis.html)

```{r}
ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, color = "red", size = 1)
```

### 23.3.2 Residuals

Residuals are the amounts of data that the model has not captured.
They're just the distances between the observed and predicted
values.

Add residuals to the model using modelr::add_residuals:
```{r}
sim1 <- sim1 %>%
  add_residuals(sim1_mod)
sim1
```

To understand residuals, draw frequency polygons of the residuals.
```{r}
ggplot(sim1, aes(resid)) +
  geom_freqpoly(binwidth = 0.5)
```

This tells you how far away from the data is your model.
Note that the average of your residuals will always be 0.
Usually, you want to re-create plots using just the residuals.
```{r}
ggplot(sim1, aes(x, resid)) +
  geom_ref_line(h = 0) + # draw thick horiz reference line at 0.
                         # v = 5 to draw vertical ref line at 5.
  geom_point()
```
This looks like random noise, meaning our model has done a good job
of capturing the patterns in the dataset.

### 23.3.3 Exercises

1. Instead of using lm() to fit a straight line, you can use loess() to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on sim1 using loess() instead of lm(). How does the result compare to geom_smooth()?
```{r}
# Restore sim1 to original condition by dropping resid
sim1 <- sim1 %>% select(x, y)

# Fit using loess
sim1_loess_mod <- loess(y ~ x, sim1)
sim1_loess_mod

grid <- sim1 %>%
  data_grid(x) %>%
  add_predictions(sim1_loess_mod)

ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, color = "red", size = 2) +
  geom_smooth(aes(y = pred), data = grid, color = "green", size = 1)

sim1 <- sim1 %>%
  add_residuals(sim1_loess_mod)

sim1 %>%
  ggplot(aes(resid)) +
  geom_freqpoly(binwidth = 0.5)

ggplot(sim1, aes(x, resid)) +
  geom_ref_line(h = 0) +
  geom_point()
```
Now the y-axis scale is a little narrower. This means the lowess
function is a better fit. It does this by calculating a different
polynomial for each section of the input curve. If you want to
get an even better fit, you can change the span parameter.
Note though, that you shouldn't overfit, otherwise your model will
work excellently for the training dataset, and really bad for the
testing dataset.

The geom_smooth function almost exactly matches the loess model.

2. add_predictions() is paired with gather_predictions() and spread_predictions(). How do these three functions differ?

  * add_predictions mutates the dataframe by adding predictions in a
    new column called pred.
  * spread_predictions mutates the dataframe by adding one prediction
    for each model
  * gather_predictions mutates the dataframe by adding 2 columns 
    (.model, and .pred) and repeats the input rows for each model.
    
3. What does geom_ref_line() do? What package does it come from? Why is displaying a reference line in plots showing residuals useful and important?

geom_ref_line adds a reference line to the plot. If you give it the
h parameter, a horizontal line is drawn at the value of the h param.
With a v param, a vertical line is drawn at the value of the v param.

A model creates a function that splits the difference between points
above the function and points below it. The reference line is drawn
at y = 0 for residuals. If the residuals are not equally above the
line as below it, there is a bias in the model, and we should
try and stop this bias.

4. Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals?

The frequency polygon plot is a histogram of the residuals.
We expect this plot to be centered at 0, and be similar to a 
normal distribution. If it is not, our model may have bias, and we
should make sure we remove that bias before proceeding.

## 23.4 Formulas and model families

In R, formulas provide a general way of getting “special behaviour”.
Rather than evaluating the values of the variables right away, they 
capture them so they can be interpreted by the function.

R converts formulas to functions. 
y ~ x is converted to y = a_1 + a_2 * x
Give the model_matrix function a dataframe and a formula, and it will
return the columns of the matrix:
```{r}
df <- tribble(
  ~y, ~x1, ~x2,
   4,   2,   5,
   5,   1,   6
)

model_matrix(df, y ~ x1)
```
The matrix shows the intercept of 1 for each row of data.
This intercept value is multiplied by a_1.
The value x1 is multiplied by a_2, thus completing the calculation.
If you don't want the intercept, use y ~ x - 1 as the formula.

As you add more variables to the formula, the model matrix grows
as expected:
```{r}
model_matrix(df, y ~ x1 + x2)
```

For more details, see Symbolic Description of factorial models
for analysis of variance by Wilkinson and Rogers at:
[Symbolic factorial models](https://www.jstor.org/stable/2346786)

### 23.4.1 Categorical variables

A formula like y ~ sex is converted to:
y = x_0 + x_1 * sex_male where sex_male = 1 if male, 0 otherwise.

```{r}
df <- tribble(
  ~sex, ~response,
  "male",       1,
  "female",     2,
  "male",       1
)

model_matrix(df, response ~ sex)
```

Let's look at some data and models:
```{r}
ggplot(sim2) +
  geom_point(aes(x, y))
```
```{r}
mod2 <- lm(y ~ x, data = sim2)

grid <- sim2 %>%
  data_grid(x) %>%
  add_predictions(mod2)
grid
```
So a model with a categorical x will predict the mean value of
each category, because the mean minimizes the root-mean-square
distance.

```{r}
ggplot(sim2, aes(x)) +
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred), color = "red", size = 4)
```

You can't make predictions about levels you didn't observe:
```{r}
# This raises an error:
# factor x has new level e
#
# tibble(x = "e") %>%
#  add_predictions(mod2)
```

### 23.4.2 Interactions (continuous and categorical)

What happens when you combine a continuous and a categorical variable?
```{r}
ggplot(sim3, aes(x1, y)) +
  geom_point(aes(color = x2))
```

Two possible models we can use to fit this:
```{r}
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)
```

The x1 + x2 model will estimate each effect independent of the others.
The y ~ x1 * x2 model is converted to the function:
y = a_0 + a_1 * x1 + a_2 * x2 + x_12 * x1 * x2
The x_12 * x1 * x2 is the interaction term, but note that it also
contains the independent terms.

To visualize these models, we need two tricks:

  * data_grid needs to get both x1 and x2 so it can generate all
    combinations of values of the two
  * gather_predictions will update the dataframe with two columns:
    + one to show which model the prediction came from
    + second to show the value of the prediction
    
```{r}
grid <- sim3 %>%
  data_grid(x1, x2) %>%
  gather_predictions(mod1, mod2)
grid
```

Now we can plot both model results at once using facet_wrap:
```{r}
ggplot(sim3, aes(x1, color = x2)) +
  geom_point(aes(y = y), size = 2) +
  geom_line(data = grid, aes(y = pred)) +
  facet_wrap(~ model)
```

Which model is better? Let's look at the residuals.
```{r}
sim3 <- sim3 %>%
  gather_residuals(mod1, mod2)

ggplot(sim3, aes(x1, resid, color = x2)) +
  geom_point() +
  facet_grid(model ~ x2)
```
We can see for mod1 that there is some pattern in b and d.
This means mod1 did not capture this pattern.
For mod2, there is no pattern.
This means that mod2 did capture the pattern.
So we would naturally prefer mod2 to mod1.

### 23.4.3 Interactions (two continuous)
```{r}
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

sim4

grid <- sim4 %>%
  data_grid(
    x1 = seq_range(x1, n = 5), # generate a sequence over the range
                               # of x1. The output will be 5 values.
                               # The ends of the range are included.
                               # pretty = TRUE to generate a sequence
                               # pleasing to the eye.
    x2 = seq_range(x2, n = 5)
  ) %>%
  gather_predictions(mod1, mod2)
grid
```

seq_range function parameters:

  * n: specifies number of values to emit. Range ends will be a part
       of the emitted values.
  * pretty: When this is TRUE, sequence values are pleasing to the eye
  * trim = 0.1: Trims 10% of the tail values. Useful for fat-tailed
                distributions when you want to generate values in
                the center of the distribution.
  * expand = 0.1: Expands the range by 10%

```{r}
seq_range(c(0.0123, 0.923423), n = 5)
seq_range(c(0.0123, 0.923423), n = 5, pretty = TRUE)
```

```{r}
x1 <- rcauchy(100)
seq_range(x1, n = 5)
seq_range(x1, n = 5, trim = 0.10)
seq_range(x1, n = 5, trim = 0.25)
seq_range(x1, n = 5, trim = 0.50)
```

```{r}
x2 <- c(0, 1)
seq_range(x2, n = 5)
seq_range(x2, n = 5, expand = 0.10)
seq_range(x2, n = 5, expand = 0.25)
seq_range(x2, n = 5, expand = 0.50)
```

Now let's visualize the predictions as a surface with x1 and x2
being the two axes:
```{r}
ggplot(grid, aes(x1, x2)) +
  geom_tile(aes(fill = pred)) +
  facet_wrap(~ model)
```

These predictions are different, but our eyes cannot distinguish
them. Instead, let's look at the surface from the side with
multiple slices:
```{r}
ggplot(grid, aes(x1, pred, color = x2,
                 group = x2)) + # group by the bucketized values of x2
  geom_line() +
  facet_wrap(~ model)

ggplot(grid, aes(x2, pred, color = x1, group = x1)) +
  geom_line() +
  facet_wrap(~ model)
```

This interaction reveals that there is not a fixed offset.
You need to consider both the x1 and x2 values to get y.

### 23.4.4 Transformations

To use transformations inside the formula 
(ex. log(y) ~ sqrt(x1) + x2)

  * If you're using + - * ^ wrap it inside an I(), so it is not
    considered part of the model spec.
    ex. y ~ I(x ^ 2)
  * In 
    y ~ x * x + x, x * x is the interaction of x with itself,
    which is just x. So this formula becomes 
    y ~ x. 
    If you wanted to use a square term, use 
    y ~ I(x ^ 2) instead
  * Redundant terms are dropped. In 
    y ~ x * x + x, since x * x is just x, we get
    y ~ x + x, which then becomes
    y ~ x
  * If you forget the I() around x ^ 2,
    y ~ x ^ 2 + x becomes
    y ~ x * x + x which then becomes
    y ~ x + x which is
    y ~ x
You can always use the model matrix to see what equation lm is fitting
```{r}
df <- tribble(
  ~x,   ~y,
   1,    1,
   2,    2,
   3,    3
)
model_matrix(df, y ~ x^2 + x)
model_matrix(df, y ~ I(x^2) + x)
```

Taylor's theorem says that you can approximate any smooth function
with an infinite sum of polynomials. Since typing
y = a_1 + a_2 * x + a_3 * x^2 + a_4 * x ^ 3
by hand is tedious, R provides the poly function:
```{r}
model_matrix(df, y ~ poly(x, 2))
```
Unfortunately, beyond the range of your data, polynomials shoot off
to infinity. Instead, use the splines::ns function:
```{r}
library(splines)
model_matrix(df, y ~ ns(x, 2))
```

Example:
```{r}
sim5 <- tibble(
  x = seq(0, 3.5 * pi, length = 50),
  y = 4 * sin(x) + rnorm(length(x))
)

ggplot(sim5, aes(x, y)) +
  geom_point()
```

Fitting 5 models to this data:
```{r}
mod1 <- lm(y ~ ns(x, 1), data = sim5)
mod2 <- lm(y ~ ns(x, 2), data = sim5)
mod3 <- lm(y ~ ns(x, 3), data = sim5)
mod4 <- lm(y ~ ns(x, 4), data = sim5)
mod5 <- lm(y ~ ns(x, 5), data = sim5)

grid <- sim5 %>%
  data_grid(x = seq_range(x, n = 50, expand = 0.1)) %>%
  gather_predictions(mod1, mod2, mod3, mod4, mod5, .pred = "y")

ggplot(sim5, aes(x, y)) +
  geom_point() +
  geom_line(data = grid, color = "red") +
  facet_wrap(~ model)
```

Extrapolation beyond the data is pretty bad. This is the downside
of approximating a function with a model. The model can never tell you
if the behavior is true beyond your data range.

### 23.4.5 Exercises

1. What happens if you repeat the analysis of sim2 using a model without an intercept. What happens to the model equation? What happens to the predictions?
```{r}
mod1 <- lm(y ~ x, data = sim2)
mod2 <- lm(y ~ x - 1, data = sim2)

grid <- sim2 %>%
  data_grid(x) %>%
  gather_predictions(mod1, mod2)

ggplot(sim2, aes(x)) +
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred), color = "red", size = 4) +
  facet_wrap(~ model)
```
It's the same as with the intercept, since we're not using the
intercept - just finding the mean value for each categorical x
(x = a, x = b, etc.) value in the data.

2. Use model_matrix() to explore the equations generated for the models I fit to sim3 and sim4. Why is * a good shorthand for interaction?
```{r}
sim3
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)
model_matrix(sim3, y ~ x1 + x2)
model_matrix(sim3, y ~ x1 * x2)
```
Since x1 is numeric and x2 is categorical, we get 1 coefficient
for x1 and 3 coefficients for x2 (x2b, x2c, x2d) and an intercept.
The intercept is always 1. Each categorical level is converted
into x2b, x2c, x2d values being 000 for a, 100 for b, 010 for c,
and 001 for d. This makes sense, because when x2 is a, there is no
reason to have b, c, and d being represented - just a.

For y ~ x1 * x2, we get 1 coefficient for x1 and 3 coefficients for
x2 (x2b, x2c, x2d) and an intercept. We also get the interaction
variables x1:x2b, x1:x2c and x1:x2d.

* is a good shorthand for interaction because multiplying two values
is a combination to produce the output. Here also the two variables
are interacting (combining) to produce the output.

3. Using the basic principles, convert the formulas in the following two models into functions. (Hint: start by converting the categorical variable into 0-1 variables.)

mod1 <- lm(y ~ x1 + x2, data = sim3)
y = intercept + a_1 * x1 + x2 * x2

mod2 <- lm(y ~ x1 * x2, data = sim3)
y = intercept + a_1 * x1 + a_2 * x2 + a_3 * x1 * x2

4. For sim4, which of mod1 and mod2 is better? I think mod2 does a slightly better job at removing patterns, but it’s pretty subtle. Can you come up with a plot to support my claim?

```{r}
sim4

mod1 <- lm(y ~ x1 + x2, sim4)
mod2 <- lm(y ~ x1 * x2, sim4)

# Instead of using the data_grid function,
# we get a grid using the same values as x1 and x2.
# This lets us compare the predictions to the actual values.
grid <- sim4 %>%
  select(x1, x2) %>%
  gather_predictions(mod1, mod2)

grid['pred_minus_actual'] = abs(grid$pred - sim4$y)
grid

ggplot(grid, aes(x1, x2)) +
  geom_tile(aes(fill = pred_minus_actual)) +
  facet_wrap(~ model)
```
It's very difficult to see the difference between the models using
a plot. Instead, let us use the sum of absolute errors for each
model:
```{r}
grid %>%
  select(model, pred_minus_actual) %>%
  group_by(model) %>%
  summarize(sum_pred_minus_actual = sum(pred_minus_actual))
```
So yes, model 2 is better than model 1 (at least it has a better fit).

## 23.5 Missing values

R's default modelling behavior is to drop missing values silently.
By using options(na.action = na.warn) at the beginning of the code,
we specified that R should always warn when it sees a missing value.

```{r}
df <- tribble(
  ~x,   ~y,
   1,  2.2,
   2,   NA,
   3,  3.5,
   4,  8.3,
  NA,   10
)

mod <- lm(y ~ x, data = df)
```
To suppress warnings globally:
options(na.action = na.ignore)

To suppress warnings locally:
```{r}
mod <- lm(y ~ x, data = df, na.action = na.exclude)
```

nobs(mod) shows you how many observations were used. You can compare
it to the number of rows of the dataframe.
```{r}
nobs(mod)
```

## 23.6 Other model families

We focused on linear models. Linear models assume the residuals have
a normal distribution. Other models are:

  * Linear models: ex. stats::lm
    Assumes y is continuous.
    Assumes residuals (error) have a normal distribution.
  * Generalised linear models: ex. stats::glm
    y can be continuous or discrete
    They create a distance metric based on likelihood.
  * Generalised additive models: ex. mgcv::gam
    Extends glm to incorporate arbitrary smooth functions.
    So your y ~ s(x) formula becomes y = f(x) function which allows
    gam() to build smoothness into the function based on smoothness
    constraints to make the problem tractable.
  * Penalised linear models: ex. glmnet::glmnet
    Adds a penalty term based on the length of the parameter vector
    that penalizes complex models.
    This allows the model to generalise better.
  * Robust linear models: ex. MASS::rlm
    rlm models tweak the distance to downweight points that are far
    away. This makes them robust to outliers at the cost of being
    not quite so good when there are no outliers.
  * Trees: ex. rpart::rpart
    They piece a piecewise constant model, splitting the data into
    progressively smaller and smaller pieces. Trees in aggregate
    are very powerful (ex. Random forests randomForest::randomForest,
    or Gradient Boosting Machines xgboost::xgboost)
    
These models work the same as the Linear models from a programming
perspective. Once you've mastered the mechanics of one model, you can
apply the same mechanics to use other models.